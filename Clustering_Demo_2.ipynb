{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering Demo 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMybxt2fouJX/pxmKf8kgNU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/univ-3360-vu-smartcities/clustering-demo-2/blob/master/Clustering_Demo_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLhYz9rLCzrW",
        "colab_type": "text"
      },
      "source": [
        "# Clustering Algorithms\n",
        "\n",
        "In this example, we will be looking at how to implement hierarchical and density based clustering using scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHRxUQuzCw8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0a75fJ7EtWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_cluster(data, labels, plot_title):\n",
        "  plt.figure(figsize=(10,5))\n",
        "  unique_labels = set(labels)\n",
        "  core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
        "  colors = ['b','g','r','c','m','y','k']\n",
        "\n",
        "  for k in unique_labels:\n",
        "      class_member_mask = (labels == k)\n",
        "      xy = data[class_member_mask & ~core_samples_mask]\n",
        "      plt.scatter(xy[:,0], xy[:,1], color=colors[k%7])\n",
        "\n",
        "  plt.title(plot_title)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd667lM_DF_T",
        "colab_type": "text"
      },
      "source": [
        "# Data Import\n",
        "\n",
        "In this lesson, we will be working with the familar moon dataset. If you recall from the last demonstration, K Means was unable to cluster the moon data properly. Today, we will see what happens when we use hierarchical and density based clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIBB4U5aDFZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_moon_data, _ = datasets.make_moons(n_samples=1500, noise=0.05, random_state=1)\n",
        "\n",
        "plt.scatter(X_moon_data[:,0], X_moon_data[:,1], s=6)\n",
        "plt.title(\"Moon Dataset\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUxBkjpXDeLq",
        "colab_type": "text"
      },
      "source": [
        "# Agglomerative Hierarchical\n",
        "\n",
        "First we will look at hierarchical clustering, specifically agglomerative hierarchical clustering. We will use scikit-learn to easily implement hierarchical clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkEcBIR6DZso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8DODyStD3MQ",
        "colab_type": "text"
      },
      "source": [
        "Hierarchical clustering is unique in that it can cluster given a known number of clusters, or given a parameter called the distance threshold to find the number of clusters automatically. In this case, we know that there are two clusters, so we can specify this to the algorithm.\n",
        "\n",
        "Recall also that hierarchical clustering requires a parameter to determine which type of linkage to use. For this example, lets first try average linkage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzXw9TLgD2dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hier = AgglomerativeClustering(n_clusters=2, linkage='average')\n",
        "hier.fit(X_moon_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81p6aV03EvLj",
        "colab_type": "text"
      },
      "source": [
        "Now we can use the function defined earlier to plot the results of the clustering, using colors to represent the different clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WEYp1zWEpF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_cluster(X_moon_data, hier.labels_, \"Hierarchical Clustering with Average Linkage\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsE0EXXMFRd0",
        "colab_type": "text"
      },
      "source": [
        "So we can see that the algorithm did not do exactly what we wanted it to, but did produce a slightly more reasonable result than K Means. Let's now try varying the linkage parameter to see what effect it has on the resulting clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSQBTzpbFLrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hier = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "hier.fit(X_moon_data)\n",
        "plot_cluster(X_moon_data, hier.labels_, \"Hierarchical Clustering with Single Linkage\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMeCjwbGFrCV",
        "colab_type": "text"
      },
      "source": [
        "And we can see this this worked exactly as expected! This is a good demonstration of why careful hyperparamter selection is very important to data science. For hierarchical clustering, different types of linkage will often give very different results, so it is best to several types and compare their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PpGX4K6GCf1",
        "colab_type": "text"
      },
      "source": [
        "# Density Based Clustering\n",
        "\n",
        "Now we will look at density based clustering. For this example, we will specifically be using DBSCAN, which can be easily implemented with scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0gnzKY1FqB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import DBSCAN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNYezi5iGUpF",
        "colab_type": "text"
      },
      "source": [
        "As a first test, lets ry running DBSCAN with the default parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh3jzE7pGR9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = DBSCAN()\n",
        "db.fit(X_moon_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N4UOUmIGeUd",
        "colab_type": "text"
      },
      "source": [
        "And once again, we can plot the results to evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLrTkyXcGdcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_cluster(X_moon_data, db.labels_, \"DBSCAN Clustering with eps=0.5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9GgQFAEGrUk",
        "colab_type": "text"
      },
      "source": [
        "We can see that right out of the box, DBSCAN is not working very well. But it is important to remember that hyperparameter selection can make a very significant difference in performance, so lets try modifying the epsilon parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbgfWTTpGpB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = DBSCAN(eps=0.25)\n",
        "db.fit(X_moon_data)\n",
        "plot_cluster(X_moon_data, db.labels_, \"DBSCAN Clustering with eps=0.25\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMETLXwsHBbX",
        "colab_type": "text"
      },
      "source": [
        "And we can see again that with the proper hyperparameter selection, DBSCAN is very effective at separating the classes. In general, you may have to run an algorithm several times with different hyperparameters before you are able to find a good result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP4t1Z-JG9ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}